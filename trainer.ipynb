{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/ASabryMazroua/pix2pix.git"
      ],
      "metadata": {
        "id": "eBLW0822jnNK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "93259147-3a1a-4c26-e8ce-41b65f01f7fb"
      },
      "id": "eBLW0822jnNK",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'pix2pix' already exists and is not an empty directory.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.getcwd()"
      ],
      "metadata": {
        "id": "TN2aG484m7Fd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "ae5d5906-3c1b-4358-c6c6-21dd3747384a"
      },
      "id": "TN2aG484m7Fd",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mv  -v ./pix2pix/* /content/"
      ],
      "metadata": {
        "id": "U8z1KU2TmTer",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "59b365b9-1b3c-4d6f-c6c2-73fe76e1045c"
      },
      "id": "U8z1KU2TmTer",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mv: cannot stat './pix2pix/*': No such file or directory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install git+https://www.github.com/keras-team/keras-contrib.git"
      ],
      "metadata": {
        "id": "TjHBabyfkS4_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e61301e2-5979-470f-bbe1-42345d5b76b4"
      },
      "id": "TjHBabyfkS4_",
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting git+https://www.github.com/keras-team/keras-contrib.git\n",
            "  Cloning https://www.github.com/keras-team/keras-contrib.git to /tmp/pip-req-build-pmltob65\n",
            "  Running command git clone -q https://www.github.com/keras-team/keras-contrib.git /tmp/pip-req-build-pmltob65\n",
            "Requirement already satisfied: keras in /usr/local/lib/python3.7/dist-packages (from keras-contrib==2.0.8) (2.8.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "617ea046",
      "metadata": {
        "id": "617ea046"
      },
      "outputs": [],
      "source": [
        "from __future__ import print_function, division\n",
        "import scipy\n",
        "\n",
        "from keras_contrib.layers.normalization.instancenormalization import InstanceNormalization\n",
        "from keras.layers import Input, Dense, Reshape, Flatten, Dropout, Concatenate\n",
        "from keras.layers import BatchNormalization, Activation, ZeroPadding2D, LeakyReLU\n",
        "# from keras.layers.advanced_activations import LeakyReLU\n",
        "from keras.layers.convolutional import UpSampling2D, Conv2D\n",
        "from keras.models import Sequential, Model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "import datetime\n",
        "import matplotlib.pyplot as plt\n",
        "import sys\n",
        "from data_loader import DataLoader\n",
        "import numpy as np\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "c98e28e9",
      "metadata": {
        "id": "c98e28e9"
      },
      "outputs": [],
      "source": [
        "class Pix2Pix():\n",
        "    def __init__(self):\n",
        "        # Input shape\n",
        "        self.img_rows = 512\n",
        "        self.img_cols = 512 \n",
        "        self.channels = 3\n",
        "        self.img_shape = (self.img_rows, self.img_cols, self.channels)\n",
        "\n",
        "        # Configure data loader\n",
        "        self.dataset_name = 'context_id'\n",
        "        self.data_loader = DataLoader(img_res=(self.img_rows, self.img_cols))\n",
        "\n",
        "\n",
        "        # Calculate output shape of D (PatchGAN)\n",
        "        patch = int(self.img_rows / 2**4)\n",
        "        self.disc_patch = (patch, patch, 1)\n",
        "\n",
        "        # Number of filters in the first layer of G and D\n",
        "        self.gf = 64\n",
        "        self.df = 64\n",
        "\n",
        "        optimizer = Adam(0.0002, 0.5)\n",
        "\n",
        "        # Build and compile the discriminator\n",
        "        self.discriminator = self.build_discriminator()\n",
        "        self.discriminator.compile(loss='mse',\n",
        "            optimizer=optimizer,\n",
        "            metrics=['accuracy'])\n",
        "\n",
        "        #-------------------------\n",
        "        # Construct Computational\n",
        "        #   Graph of Generator\n",
        "        #-------------------------\n",
        "\n",
        "        # Build the generator\n",
        "        self.generator = self.build_generator()\n",
        "\n",
        "        # Input images and their conditioning images\n",
        "        img_A = Input(shape=self.img_shape)\n",
        "        img_B = Input(shape=self.img_shape)\n",
        "\n",
        "        # By conditioning on B generate a fake version of A\n",
        "        fake_A = self.generator(img_B)\n",
        "\n",
        "        # For the combined model we will only train the generator\n",
        "        self.discriminator.trainable = False\n",
        "\n",
        "        # Discriminators determines validity of translated images / condition pairs\n",
        "        valid = self.discriminator([fake_A, img_B])\n",
        "\n",
        "        self.combined = Model(inputs=[img_A, img_B], outputs=[valid, fake_A])\n",
        "        self.combined.compile(loss=['mse', 'mae'],\n",
        "                              loss_weights=[1, 100],\n",
        "                              optimizer=optimizer)\n",
        "\n",
        "    def build_generator(self):\n",
        "        \"\"\"U-Net Generator\"\"\"\n",
        "\n",
        "        def conv2d(layer_input, filters, f_size=4, bn=True):\n",
        "            \"\"\"Layers used during downsampling\"\"\"\n",
        "            d = Conv2D(filters, kernel_size=f_size, strides=2, padding='same')(layer_input)\n",
        "            d = LeakyReLU(alpha=0.2)(d)\n",
        "            if bn:\n",
        "                d = BatchNormalization(momentum=0.8)(d)\n",
        "            return d\n",
        "\n",
        "        def deconv2d(layer_input, skip_input, filters, f_size=4, dropout_rate=0):\n",
        "            \"\"\"Layers used during upsampling\"\"\"\n",
        "            u = UpSampling2D(size=2)(layer_input)\n",
        "            u = Conv2D(filters, kernel_size=f_size, strides=1, padding='same', activation='relu')(u)\n",
        "            if dropout_rate:\n",
        "                u = Dropout(dropout_rate)(u)\n",
        "            u = BatchNormalization(momentum=0.8)(u)\n",
        "            u = Concatenate()([u, skip_input])\n",
        "            return u\n",
        "\n",
        "        # Image input\n",
        "        d0 = Input(shape=self.img_shape)\n",
        "\n",
        "        # Downsampling\n",
        "        d1 = conv2d(d0, self.gf, bn=False)\n",
        "        d2 = conv2d(d1, self.gf*2)\n",
        "        d3 = conv2d(d2, self.gf*4)\n",
        "        d4 = conv2d(d3, self.gf*8)\n",
        "        d5 = conv2d(d4, self.gf*8)\n",
        "        d6 = conv2d(d5, self.gf*8)\n",
        "        d7 = conv2d(d6, self.gf*8)\n",
        "\n",
        "        # Upsampling\n",
        "        u1 = deconv2d(d7, d6, self.gf*8)\n",
        "        u2 = deconv2d(u1, d5, self.gf*8)\n",
        "        u3 = deconv2d(u2, d4, self.gf*8)\n",
        "        u4 = deconv2d(u3, d3, self.gf*4)\n",
        "        u5 = deconv2d(u4, d2, self.gf*2)\n",
        "        u6 = deconv2d(u5, d1, self.gf)\n",
        "\n",
        "        u7 = UpSampling2D(size=2)(u6)\n",
        "        output_img = Conv2D(self.channels, kernel_size=4, strides=1, padding='same', activation='tanh')(u7)\n",
        "\n",
        "        return Model(d0, output_img)\n",
        "\n",
        "    def build_discriminator(self):\n",
        "\n",
        "        def d_layer(layer_input, filters, f_size=4, bn=True):\n",
        "            \"\"\"Discriminator layer\"\"\"\n",
        "            d = Conv2D(filters, kernel_size=f_size, strides=2, padding='same')(layer_input)\n",
        "            d = LeakyReLU(alpha=0.2)(d)\n",
        "            if bn:\n",
        "                d = BatchNormalization(momentum=0.8)(d)\n",
        "            return d\n",
        "\n",
        "        img_A = Input(shape=self.img_shape)\n",
        "        img_B = Input(shape=self.img_shape)\n",
        "\n",
        "        # Concatenate image and conditioning image by channels to produce input\n",
        "        combined_imgs = Concatenate(axis=-1)([img_A, img_B])\n",
        "\n",
        "        d1 = d_layer(combined_imgs, self.df, bn=False)\n",
        "        d2 = d_layer(d1, self.df*2)\n",
        "        d3 = d_layer(d2, self.df*4)\n",
        "        d4 = d_layer(d3, self.df*8)\n",
        "\n",
        "        validity = Conv2D(1, kernel_size=4, strides=1, padding='same')(d4)\n",
        "\n",
        "        return Model([img_A, img_B], validity)\n",
        "\n",
        "    def train(self, epochs, batch_size=1, sample_interval=50):\n",
        "\n",
        "        start_time = datetime.datetime.now()\n",
        "\n",
        "        # Adversarial loss ground truths\n",
        "        valid = np.ones((batch_size,) + self.disc_patch)\n",
        "        fake = np.zeros((batch_size,) + self.disc_patch)\n",
        "\n",
        "        for epoch in range(epochs):\n",
        "            for batch_i, (imgs_A, imgs_B) in enumerate(self.data_loader.load_batch(batch_size)):\n",
        "\n",
        "                # ---------------------\n",
        "                #  Train Discriminator\n",
        "                # ---------------------\n",
        "\n",
        "                # Condition on B and generate a translated version\n",
        "                fake_A = self.generator.predict(imgs_B)\n",
        "\n",
        "                # Train the discriminators (original images = real / generated = Fake)\n",
        "                d_loss_real = self.discriminator.train_on_batch([imgs_A, imgs_B], valid)\n",
        "                d_loss_fake = self.discriminator.train_on_batch([fake_A, imgs_B], fake)\n",
        "                d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
        "\n",
        "                # -----------------\n",
        "                #  Train Generator\n",
        "                # -----------------\n",
        "\n",
        "                # Train the generators\n",
        "                g_loss = self.combined.train_on_batch([imgs_A, imgs_B], [valid, imgs_A])\n",
        "\n",
        "                elapsed_time = datetime.datetime.now() - start_time\n",
        "                # Plot the progress\n",
        "                print (\"[Epoch %d/%d] [Batch %d/%d] [D loss: %f, acc: %3d%%] [G loss: %f] time: %s\" % (epoch, epochs,\n",
        "                                                                        batch_i, self.data_loader.n_batches,\n",
        "                                                                        d_loss[0], 100*d_loss[1],\n",
        "                                                                        g_loss[0],\n",
        "                                                                        elapsed_time))\n",
        "\n",
        "                # If at save interval => save generated image samples\n",
        "                if batch_i % sample_interval == 0:\n",
        "                    self.sample_images(epoch, batch_i)\n",
        "\n",
        "    def sample_images(self, epoch, batch_i):\n",
        "        os.makedirs('images/%s' % self.dataset_name, exist_ok=True)\n",
        "        r, c = 3, 3\n",
        "\n",
        "        imgs_A, imgs_B = self.data_loader.load_data(batch_size=3, is_testing=True)\n",
        "        fake_A = self.generator.predict(imgs_B)\n",
        "\n",
        "        gen_imgs = np.concatenate([imgs_B, fake_A, imgs_A])\n",
        "\n",
        "        # Rescale images 0 - 1\n",
        "        gen_imgs = 0.5 * gen_imgs + 0.5\n",
        "\n",
        "        titles = ['Condition', 'Generated', 'Original']\n",
        "        fig, axs = plt.subplots(r, c)\n",
        "        cnt = 0\n",
        "        for i in range(r):\n",
        "            for j in range(c):\n",
        "                axs[i,j].imshow(gen_imgs[cnt])\n",
        "                axs[i, j].set_title(titles[i])\n",
        "                axs[i,j].axis('off')\n",
        "                cnt += 1\n",
        "        fig.savefig(\"images/%s/%d_%d.png\" % (self.dataset_name, epoch, batch_i))\n",
        "        plt.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4148400a",
      "metadata": {
        "id": "4148400a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "05bc07d2-852b-41ec-d0e7-3938afb137d6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Epoch 0/4] [Batch 0/803] [D loss: 4.977964, acc:  33%] [G loss: 85.551239] time: 0:00:19.243707\n",
            "[Epoch 0/4] [Batch 1/803] [D loss: 31.465884, acc:  54%] [G loss: 69.712372] time: 0:00:23.861227\n",
            "[Epoch 0/4] [Batch 2/803] [D loss: 29.884064, acc:  40%] [G loss: 66.501884] time: 0:00:24.334517\n",
            "[Epoch 0/4] [Batch 3/803] [D loss: 10.026969, acc:  49%] [G loss: 59.002930] time: 0:00:24.879723\n",
            "[Epoch 0/4] [Batch 4/803] [D loss: 6.354974, acc:  53%] [G loss: 52.702618] time: 0:00:25.341565\n",
            "[Epoch 0/4] [Batch 5/803] [D loss: 2.711073, acc:  49%] [G loss: 58.847103] time: 0:00:25.895308\n",
            "[Epoch 0/4] [Batch 6/803] [D loss: 1.296492, acc:  53%] [G loss: 46.915943] time: 0:00:26.365985\n",
            "[Epoch 0/4] [Batch 7/803] [D loss: 1.156330, acc:  50%] [G loss: 66.456718] time: 0:00:26.874251\n",
            "[Epoch 0/4] [Batch 8/803] [D loss: 1.005461, acc:  45%] [G loss: 49.541080] time: 0:00:27.341531\n",
            "[Epoch 0/4] [Batch 9/803] [D loss: 0.887206, acc:  51%] [G loss: 75.605782] time: 0:00:27.806137\n",
            "[Epoch 0/4] [Batch 10/803] [D loss: 0.837507, acc:  49%] [G loss: 41.418430] time: 0:00:28.293243\n",
            "[Epoch 0/4] [Batch 11/803] [D loss: 0.788814, acc:  47%] [G loss: 59.635033] time: 0:00:28.801294\n",
            "[Epoch 0/4] [Batch 12/803] [D loss: 0.617211, acc:  49%] [G loss: 56.336117] time: 0:00:29.268326\n",
            "[Epoch 0/4] [Batch 13/803] [D loss: 0.600071, acc:  54%] [G loss: 53.059258] time: 0:00:29.750687\n",
            "[Epoch 0/4] [Batch 14/803] [D loss: 0.526697, acc:  53%] [G loss: 40.158577] time: 0:00:30.221490\n",
            "[Epoch 0/4] [Batch 15/803] [D loss: 0.578265, acc:  53%] [G loss: 55.930885] time: 0:00:30.692666\n",
            "[Epoch 0/4] [Batch 16/803] [D loss: 0.503173, acc:  52%] [G loss: 40.851379] time: 0:00:31.169929\n",
            "[Epoch 0/4] [Batch 17/803] [D loss: 0.550313, acc:  50%] [G loss: 46.496181] time: 0:00:31.645145\n",
            "[Epoch 0/4] [Batch 18/803] [D loss: 0.567121, acc:  46%] [G loss: 92.748833] time: 0:00:32.118364\n",
            "[Epoch 0/4] [Batch 19/803] [D loss: 0.529812, acc:  50%] [G loss: 48.428116] time: 0:00:32.674810\n",
            "[Epoch 0/4] [Batch 20/803] [D loss: 0.469640, acc:  51%] [G loss: 65.274338] time: 0:00:33.143569\n",
            "[Epoch 0/4] [Batch 21/803] [D loss: 0.614849, acc:  49%] [G loss: 40.505562] time: 0:00:33.620565\n",
            "[Epoch 0/4] [Batch 22/803] [D loss: 0.532593, acc:  54%] [G loss: 49.036739] time: 0:00:34.094501\n",
            "[Epoch 0/4] [Batch 23/803] [D loss: 0.601008, acc:  50%] [G loss: 55.382694] time: 0:00:34.568366\n",
            "[Epoch 0/4] [Batch 24/803] [D loss: 0.484998, acc:  52%] [G loss: 48.244560] time: 0:00:35.049468\n",
            "[Epoch 0/4] [Batch 25/803] [D loss: 0.541464, acc:  50%] [G loss: 39.668530] time: 0:00:35.515521\n",
            "[Epoch 0/4] [Batch 26/803] [D loss: 0.488438, acc:  48%] [G loss: 46.503822] time: 0:00:35.989804\n",
            "[Epoch 0/4] [Batch 27/803] [D loss: 0.444191, acc:  51%] [G loss: 72.899895] time: 0:00:36.466836\n",
            "[Epoch 0/4] [Batch 28/803] [D loss: 0.383023, acc:  54%] [G loss: 42.933365] time: 0:00:36.976364\n",
            "[Epoch 0/4] [Batch 29/803] [D loss: 0.406685, acc:  54%] [G loss: 45.721447] time: 0:00:37.459312\n",
            "[Epoch 0/4] [Batch 30/803] [D loss: 0.350460, acc:  55%] [G loss: 40.146706] time: 0:00:37.936850\n",
            "[Epoch 0/4] [Batch 31/803] [D loss: 0.463220, acc:  59%] [G loss: 53.132572] time: 0:00:38.413098\n",
            "[Epoch 0/4] [Batch 32/803] [D loss: 0.447315, acc:  49%] [G loss: 48.668781] time: 0:00:38.891859\n",
            "[Epoch 0/4] [Batch 33/803] [D loss: 0.419192, acc:  49%] [G loss: 47.030964] time: 0:00:39.371154\n",
            "[Epoch 0/4] [Batch 34/803] [D loss: 0.411723, acc:  51%] [G loss: 41.806896] time: 0:00:39.840258\n",
            "[Epoch 0/4] [Batch 35/803] [D loss: 0.415486, acc:  56%] [G loss: 37.767445] time: 0:00:40.315730\n",
            "[Epoch 0/4] [Batch 36/803] [D loss: 0.410969, acc:  56%] [G loss: 48.278282] time: 0:00:40.795389\n",
            "[Epoch 0/4] [Batch 37/803] [D loss: 0.396385, acc:  49%] [G loss: 67.210464] time: 0:00:41.300863\n",
            "[Epoch 0/4] [Batch 38/803] [D loss: 0.440178, acc:  50%] [G loss: 36.863373] time: 0:00:41.767081\n",
            "[Epoch 0/4] [Batch 39/803] [D loss: 0.422020, acc:  46%] [G loss: 51.457787] time: 0:00:42.245439\n",
            "[Epoch 0/4] [Batch 40/803] [D loss: 0.488233, acc:  51%] [G loss: 55.508049] time: 0:00:42.799824\n",
            "[Epoch 0/4] [Batch 41/803] [D loss: 0.477288, acc:  53%] [G loss: 57.030006] time: 0:00:43.270488\n",
            "[Epoch 0/4] [Batch 42/803] [D loss: 0.494182, acc:  48%] [G loss: 49.396690] time: 0:00:43.747333\n",
            "[Epoch 0/4] [Batch 43/803] [D loss: 0.429026, acc:  49%] [G loss: 41.114586] time: 0:00:44.224349\n",
            "[Epoch 0/4] [Batch 44/803] [D loss: 0.409540, acc:  53%] [G loss: 44.796993] time: 0:00:44.768558\n",
            "[Epoch 0/4] [Batch 45/803] [D loss: 0.335473, acc:  55%] [G loss: 47.145111] time: 0:00:45.248095\n",
            "[Epoch 0/4] [Batch 46/803] [D loss: 0.327364, acc:  57%] [G loss: 47.341324] time: 0:00:45.724535\n",
            "[Epoch 0/4] [Batch 47/803] [D loss: 0.349494, acc:  50%] [G loss: 59.238052] time: 0:00:46.201563\n",
            "[Epoch 0/4] [Batch 48/803] [D loss: 0.316149, acc:  57%] [G loss: 52.119354] time: 0:00:46.762765\n",
            "[Epoch 0/4] [Batch 49/803] [D loss: 0.338917, acc:  57%] [G loss: 44.876328] time: 0:00:47.242339\n",
            "[Epoch 0/4] [Batch 50/803] [D loss: 0.335775, acc:  54%] [G loss: 52.690441] time: 0:00:47.717880\n",
            "[Epoch 0/4] [Batch 51/803] [D loss: 0.374506, acc:  44%] [G loss: 54.387852] time: 0:00:48.271449\n",
            "[Epoch 0/4] [Batch 52/803] [D loss: 0.363117, acc:  59%] [G loss: 40.809650] time: 0:00:48.745580\n",
            "[Epoch 0/4] [Batch 53/803] [D loss: 0.333815, acc:  51%] [G loss: 38.587284] time: 0:00:49.215304\n",
            "[Epoch 0/4] [Batch 54/803] [D loss: 0.368497, acc:  57%] [G loss: 50.766441] time: 0:00:49.687449\n",
            "[Epoch 0/4] [Batch 55/803] [D loss: 0.331385, acc:  56%] [G loss: 46.090996] time: 0:00:50.164424\n",
            "[Epoch 0/4] [Batch 56/803] [D loss: 0.263934, acc:  64%] [G loss: 58.352009] time: 0:00:50.635137\n",
            "[Epoch 0/4] [Batch 57/803] [D loss: 0.252981, acc:  61%] [G loss: 50.174698] time: 0:00:51.154492\n",
            "[Epoch 0/4] [Batch 58/803] [D loss: 0.278093, acc:  61%] [G loss: 43.634781] time: 0:00:51.639955\n",
            "[Epoch 0/4] [Batch 59/803] [D loss: 0.336397, acc:  50%] [G loss: 58.329700] time: 0:00:52.117975\n",
            "[Epoch 0/4] [Batch 60/803] [D loss: 0.279697, acc:  56%] [G loss: 40.397839] time: 0:00:52.663889\n",
            "[Epoch 0/4] [Batch 61/803] [D loss: 0.317959, acc:  54%] [G loss: 63.067471] time: 0:00:53.148738\n",
            "[Epoch 0/4] [Batch 62/803] [D loss: 0.480888, acc:  53%] [G loss: 53.739372] time: 0:00:53.627315\n",
            "[Epoch 0/4] [Batch 63/803] [D loss: 0.895786, acc:  51%] [G loss: 54.972233] time: 0:00:54.106167\n",
            "[Epoch 0/4] [Batch 64/803] [D loss: 1.432591, acc:  56%] [G loss: 50.216026] time: 0:00:54.653140\n",
            "[Epoch 0/4] [Batch 65/803] [D loss: 1.818387, acc:  51%] [G loss: 50.555843] time: 0:00:55.129492\n",
            "[Epoch 0/4] [Batch 66/803] [D loss: 0.734749, acc:  50%] [G loss: 54.775410] time: 0:00:55.596991\n",
            "[Epoch 0/4] [Batch 67/803] [D loss: 0.560491, acc:  54%] [G loss: 47.068832] time: 0:00:56.158194\n",
            "[Epoch 0/4] [Batch 68/803] [D loss: 0.513569, acc:  48%] [G loss: 55.494614] time: 0:00:56.630513\n",
            "[Epoch 0/4] [Batch 69/803] [D loss: 0.462609, acc:  54%] [G loss: 51.349125] time: 0:00:57.184245\n",
            "[Epoch 0/4] [Batch 70/803] [D loss: 0.556619, acc:  40%] [G loss: 42.004250] time: 0:00:57.734175\n",
            "[Epoch 0/4] [Batch 71/803] [D loss: 0.498763, acc:  59%] [G loss: 59.233425] time: 0:00:58.205339\n",
            "[Epoch 0/4] [Batch 72/803] [D loss: 0.428384, acc:  58%] [G loss: 42.389423] time: 0:00:58.675984\n",
            "[Epoch 0/4] [Batch 73/803] [D loss: 0.385796, acc:  57%] [G loss: 57.664856] time: 0:00:59.159814\n",
            "[Epoch 0/4] [Batch 74/803] [D loss: 0.295159, acc:  51%] [G loss: 37.526577] time: 0:00:59.717956\n",
            "[Epoch 0/4] [Batch 75/803] [D loss: 0.313413, acc:  53%] [G loss: 45.674488] time: 0:01:00.454771\n",
            "[Epoch 0/4] [Batch 76/803] [D loss: 0.282631, acc:  57%] [G loss: 49.190216] time: 0:01:01.174402\n",
            "[Epoch 0/4] [Batch 77/803] [D loss: 0.270530, acc:  53%] [G loss: 48.984165] time: 0:01:01.750374\n",
            "[Epoch 0/4] [Batch 78/803] [D loss: 0.299727, acc:  61%] [G loss: 42.367432] time: 0:01:02.394746\n",
            "[Epoch 0/4] [Batch 79/803] [D loss: 0.301973, acc:  61%] [G loss: 42.557926] time: 0:01:02.874783\n",
            "[Epoch 0/4] [Batch 80/803] [D loss: 0.280594, acc:  61%] [G loss: 67.891685] time: 0:01:03.433119\n",
            "[Epoch 0/4] [Batch 81/803] [D loss: 0.327890, acc:  57%] [G loss: 51.835365] time: 0:01:03.977446\n",
            "[Epoch 0/4] [Batch 82/803] [D loss: 0.290906, acc:  57%] [G loss: 61.930058] time: 0:01:04.529477\n",
            "[Epoch 0/4] [Batch 83/803] [D loss: 0.322268, acc:  58%] [G loss: 42.485298] time: 0:01:05.008288\n",
            "[Epoch 0/4] [Batch 84/803] [D loss: 0.286055, acc:  55%] [G loss: 49.307899] time: 0:01:05.486184\n",
            "[Epoch 0/4] [Batch 85/803] [D loss: 0.290145, acc:  62%] [G loss: 46.947472] time: 0:01:05.965869\n",
            "[Epoch 0/4] [Batch 86/803] [D loss: 0.252688, acc:  56%] [G loss: 34.049759] time: 0:01:06.443979\n",
            "[Epoch 0/4] [Batch 87/803] [D loss: 0.275354, acc:  62%] [G loss: 42.277302] time: 0:01:06.930052\n",
            "[Epoch 0/4] [Batch 88/803] [D loss: 0.272567, acc:  58%] [G loss: 41.515564] time: 0:01:07.438779\n",
            "[Epoch 0/4] [Batch 89/803] [D loss: 0.271428, acc:  59%] [G loss: 35.081257] time: 0:01:07.913406\n",
            "[Epoch 0/4] [Batch 90/803] [D loss: 0.264460, acc:  54%] [G loss: 40.546692] time: 0:01:08.392109\n",
            "[Epoch 0/4] [Batch 91/803] [D loss: 0.248887, acc:  61%] [G loss: 45.476284] time: 0:01:08.872598\n",
            "[Epoch 0/4] [Batch 92/803] [D loss: 0.259471, acc:  62%] [G loss: 39.503407] time: 0:01:09.351752\n",
            "[Epoch 0/4] [Batch 93/803] [D loss: 0.260640, acc:  58%] [G loss: 64.889343] time: 0:01:09.836105\n",
            "[Epoch 0/4] [Batch 94/803] [D loss: 0.273406, acc:  58%] [G loss: 48.009583] time: 0:01:10.313135\n",
            "[Epoch 0/4] [Batch 95/803] [D loss: 0.234600, acc:  62%] [G loss: 46.928787] time: 0:01:10.786708\n",
            "[Epoch 0/4] [Batch 96/803] [D loss: 0.275443, acc:  58%] [G loss: 53.624275] time: 0:01:11.266961\n",
            "[Epoch 0/4] [Batch 97/803] [D loss: 0.250392, acc:  64%] [G loss: 51.518475] time: 0:01:11.747166\n",
            "[Epoch 0/4] [Batch 98/803] [D loss: 0.241218, acc:  60%] [G loss: 38.174385] time: 0:01:12.303156\n",
            "[Epoch 0/4] [Batch 99/803] [D loss: 0.250208, acc:  63%] [G loss: 39.100666] time: 0:01:12.851144\n",
            "[Epoch 0/4] [Batch 100/803] [D loss: 0.245381, acc:  62%] [G loss: 36.410343] time: 0:01:13.335764\n",
            "[Epoch 0/4] [Batch 101/803] [D loss: 0.258117, acc:  66%] [G loss: 36.594288] time: 0:01:13.888652\n",
            "[Epoch 0/4] [Batch 102/803] [D loss: 0.206939, acc:  64%] [G loss: 57.430588] time: 0:01:14.364066\n",
            "[Epoch 0/4] [Batch 103/803] [D loss: 0.217482, acc:  70%] [G loss: 47.736862] time: 0:01:14.842157\n",
            "[Epoch 0/4] [Batch 104/803] [D loss: 0.247509, acc:  66%] [G loss: 50.913879] time: 0:01:15.317827\n",
            "[Epoch 0/4] [Batch 105/803] [D loss: 0.237760, acc:  66%] [G loss: 37.922161] time: 0:01:15.801135\n",
            "[Epoch 0/4] [Batch 106/803] [D loss: 0.207462, acc:  69%] [G loss: 55.513798] time: 0:01:16.362649\n",
            "[Epoch 0/4] [Batch 107/803] [D loss: 0.234223, acc:  65%] [G loss: 34.487160] time: 0:01:16.835566\n",
            "[Epoch 0/4] [Batch 108/803] [D loss: 0.208970, acc:  64%] [G loss: 35.870815] time: 0:01:17.314129\n",
            "[Epoch 0/4] [Batch 109/803] [D loss: 0.265828, acc:  54%] [G loss: 54.698517] time: 0:01:17.799773\n",
            "[Epoch 0/4] [Batch 110/803] [D loss: 0.216680, acc:  70%] [G loss: 40.233452] time: 0:01:18.438589\n",
            "[Epoch 0/4] [Batch 111/803] [D loss: 0.256105, acc:  68%] [G loss: 40.951660] time: 0:01:18.916288\n",
            "[Epoch 0/4] [Batch 112/803] [D loss: 0.233102, acc:  62%] [G loss: 45.587307] time: 0:01:19.469029\n",
            "[Epoch 0/4] [Batch 113/803] [D loss: 0.294906, acc:  54%] [G loss: 43.765911] time: 0:01:20.022606\n",
            "[Epoch 0/4] [Batch 114/803] [D loss: 0.276405, acc:  47%] [G loss: 41.894253] time: 0:01:20.510403\n",
            "[Epoch 0/4] [Batch 115/803] [D loss: 0.297219, acc:  55%] [G loss: 55.452408] time: 0:01:21.068703\n",
            "[Epoch 0/4] [Batch 116/803] [D loss: 0.219904, acc:  64%] [G loss: 43.355515] time: 0:01:21.557961\n",
            "[Epoch 0/4] [Batch 117/803] [D loss: 0.216986, acc:  67%] [G loss: 64.011116] time: 0:01:22.106036\n",
            "[Epoch 0/4] [Batch 118/803] [D loss: 0.266418, acc:  60%] [G loss: 43.335358] time: 0:01:22.582767\n",
            "[Epoch 0/4] [Batch 119/803] [D loss: 0.256978, acc:  64%] [G loss: 34.036053] time: 0:01:23.062561\n",
            "[Epoch 0/4] [Batch 120/803] [D loss: 0.260506, acc:  63%] [G loss: 41.370934] time: 0:01:23.614818\n",
            "[Epoch 0/4] [Batch 121/803] [D loss: 0.253390, acc:  60%] [G loss: 58.900814] time: 0:01:24.164682\n",
            "[Epoch 0/4] [Batch 122/803] [D loss: 0.228799, acc:  63%] [G loss: 44.506245] time: 0:01:24.718742\n",
            "[Epoch 0/4] [Batch 123/803] [D loss: 0.255228, acc:  61%] [G loss: 30.872715] time: 0:01:25.276292\n",
            "[Epoch 0/4] [Batch 124/803] [D loss: 0.246467, acc:  56%] [G loss: 42.118690] time: 0:01:25.831598\n",
            "[Epoch 0/4] [Batch 125/803] [D loss: 0.272567, acc:  51%] [G loss: 45.036064] time: 0:01:26.309613\n",
            "[Epoch 0/4] [Batch 126/803] [D loss: 0.269074, acc:  62%] [G loss: 42.243282] time: 0:01:26.868601\n",
            "[Epoch 0/4] [Batch 127/803] [D loss: 0.201797, acc:  71%] [G loss: 48.049225] time: 0:01:27.344079\n",
            "[Epoch 0/4] [Batch 128/803] [D loss: 0.192859, acc:  75%] [G loss: 34.774906] time: 0:01:27.822501\n",
            "[Epoch 0/4] [Batch 129/803] [D loss: 0.308127, acc:  51%] [G loss: 44.641560] time: 0:01:28.303578\n",
            "[Epoch 0/4] [Batch 130/803] [D loss: 0.251837, acc:  62%] [G loss: 40.688240] time: 0:01:28.870682\n",
            "[Epoch 0/4] [Batch 131/803] [D loss: 0.229752, acc:  62%] [G loss: 43.428104] time: 0:01:29.343348\n",
            "[Epoch 0/4] [Batch 132/803] [D loss: 0.204850, acc:  73%] [G loss: 40.460430] time: 0:01:29.895657\n",
            "[Epoch 0/4] [Batch 133/803] [D loss: 0.248707, acc:  54%] [G loss: 29.354761] time: 0:01:30.367303\n",
            "[Epoch 0/4] [Batch 134/803] [D loss: 0.215529, acc:  67%] [G loss: 35.480885] time: 0:01:30.923696\n",
            "[Epoch 0/4] [Batch 135/803] [D loss: 0.232132, acc:  68%] [G loss: 42.952847] time: 0:01:31.478312\n",
            "[Epoch 0/4] [Batch 136/803] [D loss: 0.182167, acc:  77%] [G loss: 53.558723] time: 0:01:31.974317\n",
            "[Epoch 0/4] [Batch 137/803] [D loss: 0.231661, acc:  67%] [G loss: 39.549335] time: 0:01:32.458181\n",
            "[Epoch 0/4] [Batch 138/803] [D loss: 0.261571, acc:  62%] [G loss: 37.477180] time: 0:01:32.938503\n",
            "[Epoch 0/4] [Batch 139/803] [D loss: 0.204994, acc:  72%] [G loss: 40.627590] time: 0:01:33.420574\n",
            "[Epoch 0/4] [Batch 140/803] [D loss: 0.188062, acc:  73%] [G loss: 37.170235] time: 0:01:33.897256\n",
            "[Epoch 0/4] [Batch 141/803] [D loss: 0.206269, acc:  71%] [G loss: 45.893295] time: 0:01:34.370334\n",
            "[Epoch 0/4] [Batch 142/803] [D loss: 0.231874, acc:  67%] [G loss: 45.022831] time: 0:01:34.930580\n",
            "[Epoch 0/4] [Batch 143/803] [D loss: 0.229981, acc:  69%] [G loss: 72.497795] time: 0:01:35.411555\n",
            "[Epoch 0/4] [Batch 144/803] [D loss: 0.246478, acc:  63%] [G loss: 39.399117] time: 0:01:35.903413\n",
            "[Epoch 0/4] [Batch 145/803] [D loss: 0.218944, acc:  70%] [G loss: 32.547104] time: 0:01:36.409442\n",
            "[Epoch 0/4] [Batch 146/803] [D loss: 0.226496, acc:  66%] [G loss: 56.828529] time: 0:01:36.973050\n",
            "[Epoch 0/4] [Batch 147/803] [D loss: 0.223021, acc:  74%] [G loss: 44.506969] time: 0:01:37.454416\n",
            "[Epoch 0/4] [Batch 148/803] [D loss: 0.197509, acc:  68%] [G loss: 45.135483] time: 0:01:37.936123\n",
            "[Epoch 0/4] [Batch 149/803] [D loss: 0.260998, acc:  62%] [G loss: 30.819851] time: 0:01:38.410526\n",
            "[Epoch 0/4] [Batch 150/803] [D loss: 0.195450, acc:  68%] [G loss: 50.555260] time: 0:01:38.891646\n",
            "[Epoch 0/4] [Batch 151/803] [D loss: 0.253472, acc:  67%] [G loss: 40.792686] time: 0:01:39.378237\n",
            "[Epoch 0/4] [Batch 152/803] [D loss: 0.249253, acc:  64%] [G loss: 64.346764] time: 0:01:39.864166\n",
            "[Epoch 0/4] [Batch 153/803] [D loss: 0.175980, acc:  79%] [G loss: 44.256916] time: 0:01:40.353110\n",
            "[Epoch 0/4] [Batch 154/803] [D loss: 0.172358, acc:  77%] [G loss: 37.270660] time: 0:01:40.835075\n",
            "[Epoch 0/4] [Batch 155/803] [D loss: 0.217809, acc:  64%] [G loss: 33.076130] time: 0:01:41.395307\n",
            "[Epoch 0/4] [Batch 156/803] [D loss: 0.138969, acc:  81%] [G loss: 48.166458] time: 0:01:41.956399\n",
            "[Epoch 0/4] [Batch 157/803] [D loss: 0.148115, acc:  80%] [G loss: 41.922199] time: 0:01:42.509743\n",
            "[Epoch 0/4] [Batch 158/803] [D loss: 0.194625, acc:  76%] [G loss: 38.858665] time: 0:01:43.079550\n",
            "[Epoch 0/4] [Batch 159/803] [D loss: 0.225871, acc:  60%] [G loss: 47.518753] time: 0:01:43.557881\n",
            "[Epoch 0/4] [Batch 160/803] [D loss: 0.192623, acc:  68%] [G loss: 67.571648] time: 0:01:44.043980\n",
            "[Epoch 0/4] [Batch 161/803] [D loss: 0.150716, acc:  79%] [G loss: 55.477524] time: 0:01:44.601819\n",
            "[Epoch 0/4] [Batch 162/803] [D loss: 0.177827, acc:  75%] [G loss: 39.591385] time: 0:01:45.117190\n",
            "[Epoch 0/4] [Batch 163/803] [D loss: 0.205099, acc:  69%] [G loss: 37.482147] time: 0:01:45.675586\n",
            "[Epoch 0/4] [Batch 164/803] [D loss: 0.196759, acc:  74%] [G loss: 35.402245] time: 0:01:46.159445\n",
            "[Epoch 0/4] [Batch 165/803] [D loss: 0.203169, acc:  73%] [G loss: 46.808681] time: 0:01:46.648316\n",
            "[Epoch 0/4] [Batch 166/803] [D loss: 0.432745, acc:  67%] [G loss: 39.626690] time: 0:01:47.138987\n",
            "[Epoch 0/4] [Batch 167/803] [D loss: 0.198087, acc:  72%] [G loss: 35.886715] time: 0:01:47.694432\n",
            "[Epoch 0/4] [Batch 168/803] [D loss: 0.345480, acc:  53%] [G loss: 52.518318] time: 0:01:48.248082\n",
            "[Epoch 0/4] [Batch 169/803] [D loss: 0.172656, acc:  76%] [G loss: 56.574795] time: 0:01:48.729417\n",
            "[Epoch 0/4] [Batch 170/803] [D loss: 0.164394, acc:  75%] [G loss: 36.687141] time: 0:01:49.286287\n",
            "[Epoch 0/4] [Batch 171/803] [D loss: 0.193699, acc:  73%] [G loss: 55.418766] time: 0:01:49.771667\n",
            "[Epoch 0/4] [Batch 172/803] [D loss: 0.175641, acc:  81%] [G loss: 36.392025] time: 0:01:50.331872\n",
            "[Epoch 0/4] [Batch 173/803] [D loss: 0.333581, acc:  65%] [G loss: 55.378403] time: 0:01:50.820003\n",
            "[Epoch 0/4] [Batch 174/803] [D loss: 0.397272, acc:  73%] [G loss: 53.624451] time: 0:01:51.312048\n",
            "[Epoch 0/4] [Batch 175/803] [D loss: 0.455843, acc:  62%] [G loss: 52.765842] time: 0:01:51.877021\n",
            "[Epoch 0/4] [Batch 176/803] [D loss: 0.334219, acc:  52%] [G loss: 37.594933] time: 0:01:52.436068\n",
            "[Epoch 0/4] [Batch 177/803] [D loss: 0.208923, acc:  71%] [G loss: 54.660751] time: 0:01:52.996205\n",
            "[Epoch 0/4] [Batch 178/803] [D loss: 0.287982, acc:  71%] [G loss: 40.075737] time: 0:01:53.551879\n",
            "[Epoch 0/4] [Batch 179/803] [D loss: 0.323038, acc:  54%] [G loss: 45.075630] time: 0:01:54.115050\n",
            "[Epoch 0/4] [Batch 180/803] [D loss: 0.184243, acc:  73%] [G loss: 37.556316] time: 0:01:54.673398\n",
            "[Epoch 0/4] [Batch 181/803] [D loss: 0.208611, acc:  77%] [G loss: 35.585327] time: 0:01:55.160965\n",
            "[Epoch 0/4] [Batch 182/803] [D loss: 0.138823, acc:  85%] [G loss: 43.393414] time: 0:01:55.649084\n",
            "[Epoch 0/4] [Batch 183/803] [D loss: 0.154353, acc:  80%] [G loss: 37.748322] time: 0:01:56.139252\n",
            "[Epoch 0/4] [Batch 184/803] [D loss: 0.155646, acc:  75%] [G loss: 32.709919] time: 0:01:56.625418\n",
            "[Epoch 0/4] [Batch 185/803] [D loss: 0.115454, acc:  88%] [G loss: 51.565483] time: 0:01:57.192759\n",
            "[Epoch 0/4] [Batch 186/803] [D loss: 0.123874, acc:  86%] [G loss: 45.708405] time: 0:01:57.750453\n",
            "[Epoch 0/4] [Batch 187/803] [D loss: 0.098900, acc:  92%] [G loss: 43.842613] time: 0:01:58.247283\n",
            "[Epoch 0/4] [Batch 188/803] [D loss: 0.145679, acc:  80%] [G loss: 41.460262] time: 0:01:58.816464\n",
            "[Epoch 0/4] [Batch 189/803] [D loss: 0.368744, acc:  42%] [G loss: 43.095726] time: 0:01:59.376412\n",
            "[Epoch 0/4] [Batch 190/803] [D loss: 0.402760, acc:  39%] [G loss: 48.990768] time: 0:01:59.860472\n",
            "[Epoch 0/4] [Batch 191/803] [D loss: 0.297036, acc:  64%] [G loss: 56.877525] time: 0:02:00.418592\n",
            "[Epoch 0/4] [Batch 192/803] [D loss: 0.299719, acc:  62%] [G loss: 42.270870] time: 0:02:00.976567\n",
            "[Epoch 0/4] [Batch 193/803] [D loss: 0.233285, acc:  66%] [G loss: 41.616665] time: 0:02:01.465612\n",
            "[Epoch 0/4] [Batch 194/803] [D loss: 0.245703, acc:  64%] [G loss: 44.309139] time: 0:02:01.950325\n",
            "[Epoch 0/4] [Batch 195/803] [D loss: 0.276401, acc:  52%] [G loss: 35.861946] time: 0:02:02.524504\n",
            "[Epoch 0/4] [Batch 196/803] [D loss: 0.263869, acc:  64%] [G loss: 39.180252] time: 0:02:03.157303\n",
            "[Epoch 0/4] [Batch 197/803] [D loss: 0.144097, acc:  82%] [G loss: 44.650841] time: 0:02:03.668026\n",
            "[Epoch 0/4] [Batch 198/803] [D loss: 0.148697, acc:  80%] [G loss: 32.704109] time: 0:02:04.223786\n",
            "[Epoch 0/4] [Batch 199/803] [D loss: 0.156266, acc:  83%] [G loss: 50.089417] time: 0:02:04.779594\n",
            "[Epoch 0/4] [Batch 200/803] [D loss: 0.125038, acc:  84%] [G loss: 42.968269] time: 0:02:05.257359\n",
            "[Epoch 0/4] [Batch 201/803] [D loss: 0.123015, acc:  85%] [G loss: 51.871204] time: 0:02:06.619330\n",
            "[Epoch 0/4] [Batch 202/803] [D loss: 0.228999, acc:  65%] [G loss: 43.091366] time: 0:02:07.105511\n",
            "[Epoch 0/4] [Batch 203/803] [D loss: 0.131121, acc:  88%] [G loss: 44.875355] time: 0:02:07.593147\n",
            "[Epoch 0/4] [Batch 204/803] [D loss: 0.195056, acc:  74%] [G loss: 35.339706] time: 0:02:08.086194\n",
            "[Epoch 0/4] [Batch 205/803] [D loss: 0.090933, acc:  90%] [G loss: 44.668900] time: 0:02:08.576196\n",
            "[Epoch 0/4] [Batch 206/803] [D loss: 0.089167, acc:  89%] [G loss: 34.076122] time: 0:02:09.136739\n",
            "[Epoch 0/4] [Batch 207/803] [D loss: 0.127474, acc:  94%] [G loss: 40.064625] time: 0:02:09.625534\n",
            "[Epoch 0/4] [Batch 208/803] [D loss: 0.150006, acc:  83%] [G loss: 40.903416] time: 0:02:10.187899\n",
            "[Epoch 0/4] [Batch 209/803] [D loss: 0.255591, acc:  71%] [G loss: 43.343582] time: 0:02:10.752495\n",
            "[Epoch 0/4] [Batch 210/803] [D loss: 0.121155, acc:  85%] [G loss: 62.624805] time: 0:02:11.318218\n",
            "[Epoch 0/4] [Batch 211/803] [D loss: 0.089978, acc:  92%] [G loss: 44.181335] time: 0:02:11.808214\n",
            "[Epoch 0/4] [Batch 212/803] [D loss: 0.101099, acc:  90%] [G loss: 45.284988] time: 0:02:12.298722\n",
            "[Epoch 0/4] [Batch 213/803] [D loss: 0.107013, acc:  93%] [G loss: 65.428642] time: 0:02:12.856960\n",
            "[Epoch 0/4] [Batch 214/803] [D loss: 0.396513, acc:  37%] [G loss: 38.270000] time: 0:02:13.337872\n",
            "[Epoch 0/4] [Batch 215/803] [D loss: 0.234412, acc:  61%] [G loss: 43.829460] time: 0:02:13.821908\n",
            "[Epoch 0/4] [Batch 216/803] [D loss: 0.244240, acc:  67%] [G loss: 42.701218] time: 0:02:14.308550\n",
            "[Epoch 0/4] [Batch 217/803] [D loss: 0.128740, acc:  87%] [G loss: 51.622246] time: 0:02:14.795037\n",
            "[Epoch 0/4] [Batch 218/803] [D loss: 0.146259, acc:  80%] [G loss: 40.138409] time: 0:02:15.284029\n",
            "[Epoch 0/4] [Batch 219/803] [D loss: 0.344542, acc:  72%] [G loss: 36.816990] time: 0:02:15.775257\n",
            "[Epoch 0/4] [Batch 220/803] [D loss: 0.177440, acc:  81%] [G loss: 48.359039] time: 0:02:16.274182\n",
            "[Epoch 0/4] [Batch 221/803] [D loss: 0.376618, acc:  75%] [G loss: 61.534878] time: 0:02:16.770739\n",
            "[Epoch 0/4] [Batch 222/803] [D loss: 0.278429, acc:  56%] [G loss: 55.162704] time: 0:02:17.259981\n",
            "[Epoch 0/4] [Batch 223/803] [D loss: 0.124415, acc:  89%] [G loss: 59.252071] time: 0:02:17.754881\n",
            "[Epoch 0/4] [Batch 224/803] [D loss: 0.176335, acc:  75%] [G loss: 62.334328] time: 0:02:18.241968\n",
            "[Epoch 0/4] [Batch 225/803] [D loss: 0.248758, acc:  74%] [G loss: 62.157555] time: 0:02:18.750058\n",
            "[Epoch 0/4] [Batch 226/803] [D loss: 0.746472, acc:  61%] [G loss: 40.990910] time: 0:02:19.238675\n",
            "[Epoch 0/4] [Batch 227/803] [D loss: 0.470207, acc:  55%] [G loss: 62.087997] time: 0:02:19.729581\n",
            "[Epoch 0/4] [Batch 228/803] [D loss: 0.731276, acc:  54%] [G loss: 39.094193] time: 0:02:20.218762\n",
            "[Epoch 0/4] [Batch 229/803] [D loss: 0.230773, acc:  71%] [G loss: 42.611286] time: 0:02:20.789783\n",
            "[Epoch 0/4] [Batch 230/803] [D loss: 0.167105, acc:  77%] [G loss: 44.459404] time: 0:02:21.308326\n",
            "[Epoch 0/4] [Batch 231/803] [D loss: 0.095750, acc:  90%] [G loss: 52.934677] time: 0:02:21.807610\n",
            "[Epoch 0/4] [Batch 232/803] [D loss: 0.136103, acc:  83%] [G loss: 58.095097] time: 0:02:22.296648\n",
            "[Epoch 0/4] [Batch 233/803] [D loss: 0.162525, acc:  85%] [G loss: 40.904510] time: 0:02:22.784772\n",
            "[Epoch 0/4] [Batch 234/803] [D loss: 0.214291, acc:  64%] [G loss: 50.454594] time: 0:02:23.343068\n",
            "[Epoch 0/4] [Batch 235/803] [D loss: 0.199682, acc:  72%] [G loss: 59.206768] time: 0:02:23.841427\n",
            "[Epoch 0/4] [Batch 236/803] [D loss: 0.171491, acc:  77%] [G loss: 42.651997] time: 0:02:24.401040\n",
            "[Epoch 0/4] [Batch 237/803] [D loss: 0.150289, acc:  81%] [G loss: 41.247013] time: 0:02:24.968567\n",
            "[Epoch 0/4] [Batch 238/803] [D loss: 0.127470, acc:  83%] [G loss: 57.509830] time: 0:02:25.531830\n",
            "[Epoch 0/4] [Batch 239/803] [D loss: 0.152511, acc:  78%] [G loss: 35.804047] time: 0:02:26.114640\n",
            "[Epoch 0/4] [Batch 240/803] [D loss: 0.201930, acc:  78%] [G loss: 44.878536] time: 0:02:26.605316\n",
            "[Epoch 0/4] [Batch 241/803] [D loss: 0.309299, acc:  66%] [G loss: 43.280529] time: 0:02:27.166554\n",
            "[Epoch 0/4] [Batch 242/803] [D loss: 0.155178, acc:  93%] [G loss: 48.663540] time: 0:02:27.651563\n",
            "[Epoch 0/4] [Batch 243/803] [D loss: 0.218726, acc:  66%] [G loss: 31.559937] time: 0:02:28.210039\n",
            "[Epoch 0/4] [Batch 244/803] [D loss: 0.248894, acc:  75%] [G loss: 42.508564] time: 0:02:28.775581\n",
            "[Epoch 0/4] [Batch 245/803] [D loss: 0.165208, acc:  91%] [G loss: 45.402084] time: 0:02:29.274721\n",
            "[Epoch 0/4] [Batch 246/803] [D loss: 0.121253, acc:  86%] [G loss: 61.032261] time: 0:02:29.795976\n",
            "[Epoch 0/4] [Batch 247/803] [D loss: 0.275982, acc:  71%] [G loss: 63.536186] time: 0:02:30.284532\n",
            "[Epoch 0/4] [Batch 248/803] [D loss: 0.704434, acc:  52%] [G loss: 50.411415] time: 0:02:30.845814\n",
            "[Epoch 0/4] [Batch 249/803] [D loss: 0.373555, acc:  50%] [G loss: 40.059910] time: 0:02:31.415217\n",
            "[Epoch 0/4] [Batch 250/803] [D loss: 0.460876, acc:  70%] [G loss: 38.525909] time: 0:02:31.901463\n",
            "[Epoch 0/4] [Batch 251/803] [D loss: 0.293927, acc:  74%] [G loss: 45.232849] time: 0:02:32.388206\n",
            "[Epoch 0/4] [Batch 252/803] [D loss: 0.326724, acc:  63%] [G loss: 40.447289] time: 0:02:32.874010\n",
            "[Epoch 0/4] [Batch 253/803] [D loss: 0.133018, acc:  82%] [G loss: 44.910992] time: 0:02:33.435400\n",
            "[Epoch 0/4] [Batch 254/803] [D loss: 0.184936, acc:  76%] [G loss: 38.850166] time: 0:02:33.932284\n",
            "[Epoch 0/4] [Batch 255/803] [D loss: 0.119612, acc:  88%] [G loss: 39.754509] time: 0:02:34.490427\n",
            "[Epoch 0/4] [Batch 256/803] [D loss: 0.098820, acc:  89%] [G loss: 48.168663] time: 0:02:34.982228\n",
            "[Epoch 0/4] [Batch 257/803] [D loss: 0.164919, acc:  73%] [G loss: 42.848118] time: 0:02:35.468422\n",
            "[Epoch 0/4] [Batch 258/803] [D loss: 0.187406, acc:  73%] [G loss: 34.761616] time: 0:02:35.964666\n",
            "[Epoch 0/4] [Batch 259/803] [D loss: 0.152913, acc:  83%] [G loss: 46.806858] time: 0:02:36.532697\n",
            "[Epoch 0/4] [Batch 260/803] [D loss: 0.089405, acc:  91%] [G loss: 43.713123] time: 0:02:37.020251\n",
            "[Epoch 0/4] [Batch 261/803] [D loss: 0.070612, acc:  97%] [G loss: 43.370060] time: 0:02:37.514478\n",
            "[Epoch 0/4] [Batch 262/803] [D loss: 0.117131, acc:  88%] [G loss: 39.975098] time: 0:02:38.013496\n",
            "[Epoch 0/4] [Batch 263/803] [D loss: 0.164846, acc:  76%] [G loss: 50.043697] time: 0:02:38.498543\n",
            "[Epoch 0/4] [Batch 264/803] [D loss: 0.135636, acc:  84%] [G loss: 30.017771] time: 0:02:39.061126\n",
            "[Epoch 0/4] [Batch 265/803] [D loss: 0.170273, acc:  73%] [G loss: 25.181768] time: 0:02:39.623687\n",
            "[Epoch 0/4] [Batch 266/803] [D loss: 0.084107, acc:  90%] [G loss: 35.586452] time: 0:02:40.122228\n",
            "[Epoch 0/4] [Batch 267/803] [D loss: 0.043571, acc:  98%] [G loss: 55.065342] time: 0:02:40.680988\n",
            "[Epoch 0/4] [Batch 268/803] [D loss: 0.149629, acc:  80%] [G loss: 42.898300] time: 0:02:41.255785\n",
            "[Epoch 0/4] [Batch 269/803] [D loss: 0.100738, acc:  91%] [G loss: 69.455460] time: 0:02:41.749846\n",
            "[Epoch 0/4] [Batch 270/803] [D loss: 0.063681, acc:  96%] [G loss: 57.155159] time: 0:02:42.239172\n",
            "[Epoch 0/4] [Batch 271/803] [D loss: 0.092474, acc:  94%] [G loss: 46.317883] time: 0:02:42.796259\n",
            "[Epoch 0/4] [Batch 272/803] [D loss: 0.092091, acc:  91%] [G loss: 41.090893] time: 0:02:43.363563\n",
            "[Epoch 0/4] [Batch 273/803] [D loss: 0.052277, acc:  96%] [G loss: 42.712067] time: 0:02:43.854907\n",
            "[Epoch 0/4] [Batch 274/803] [D loss: 0.096518, acc:  88%] [G loss: 40.497543] time: 0:02:44.369642\n",
            "[Epoch 0/4] [Batch 275/803] [D loss: 0.109490, acc:  88%] [G loss: 41.569889] time: 0:02:44.860017\n",
            "[Epoch 0/4] [Batch 276/803] [D loss: 0.102286, acc:  93%] [G loss: 49.080242] time: 0:02:45.347629\n",
            "[Epoch 0/4] [Batch 277/803] [D loss: 0.123167, acc:  88%] [G loss: 35.450775] time: 0:02:45.836661\n",
            "[Epoch 0/4] [Batch 278/803] [D loss: 0.593824, acc:  71%] [G loss: 33.844387] time: 0:02:46.333610\n",
            "[Epoch 0/4] [Batch 279/803] [D loss: 0.187620, acc:  73%] [G loss: 45.308861] time: 0:02:46.820606\n",
            "[Epoch 0/4] [Batch 280/803] [D loss: 0.861333, acc:  57%] [G loss: 48.677612] time: 0:02:47.387658\n",
            "[Epoch 0/4] [Batch 281/803] [D loss: 0.246592, acc:  76%] [G loss: 36.073204] time: 0:02:47.871118\n",
            "[Epoch 0/4] [Batch 282/803] [D loss: 0.658376, acc:  68%] [G loss: 45.095467] time: 0:02:48.360165\n",
            "[Epoch 0/4] [Batch 283/803] [D loss: 0.145901, acc:  85%] [G loss: 60.036476] time: 0:02:48.847691\n",
            "[Epoch 0/4] [Batch 284/803] [D loss: 0.181687, acc:  76%] [G loss: 41.897354] time: 0:02:49.412776\n",
            "[Epoch 0/4] [Batch 285/803] [D loss: 0.188533, acc:  75%] [G loss: 34.347446] time: 0:02:49.977225\n",
            "[Epoch 0/4] [Batch 286/803] [D loss: 0.285785, acc:  75%] [G loss: 40.133530] time: 0:02:50.536055\n",
            "[Epoch 0/4] [Batch 287/803] [D loss: 0.237579, acc:  80%] [G loss: 39.222816] time: 0:02:51.101415\n",
            "[Epoch 0/4] [Batch 288/803] [D loss: 0.212343, acc:  76%] [G loss: 37.652855] time: 0:02:51.668256\n",
            "[Epoch 0/4] [Batch 289/803] [D loss: 0.271392, acc:  77%] [G loss: 42.435703] time: 0:02:52.235688\n",
            "[Epoch 0/4] [Batch 290/803] [D loss: 0.134480, acc:  87%] [G loss: 50.298855] time: 0:02:52.794893\n",
            "[Epoch 0/4] [Batch 291/803] [D loss: 0.065234, acc:  97%] [G loss: 38.097176] time: 0:02:53.289203\n",
            "[Epoch 0/4] [Batch 292/803] [D loss: 0.071951, acc:  96%] [G loss: 40.178402] time: 0:02:53.855572\n",
            "[Epoch 0/4] [Batch 293/803] [D loss: 0.137389, acc:  82%] [G loss: 45.427437] time: 0:02:54.352096\n",
            "[Epoch 0/4] [Batch 294/803] [D loss: 0.090690, acc:  89%] [G loss: 45.147644] time: 0:02:54.910313\n",
            "[Epoch 0/4] [Batch 295/803] [D loss: 0.178065, acc:  78%] [G loss: 55.625404] time: 0:02:55.480759\n",
            "[Epoch 0/4] [Batch 296/803] [D loss: 0.299732, acc:  84%] [G loss: 48.126579] time: 0:02:56.040680\n",
            "[Epoch 0/4] [Batch 297/803] [D loss: 0.323236, acc:  64%] [G loss: 36.756336] time: 0:02:56.607957\n",
            "[Epoch 0/4] [Batch 298/803] [D loss: 0.208989, acc:  80%] [G loss: 44.846844] time: 0:02:57.179672\n",
            "[Epoch 0/4] [Batch 299/803] [D loss: 0.169082, acc:  78%] [G loss: 44.786777] time: 0:02:57.749859\n",
            "[Epoch 0/4] [Batch 300/803] [D loss: 0.098519, acc:  93%] [G loss: 42.977402] time: 0:02:58.319571\n",
            "[Epoch 0/4] [Batch 301/803] [D loss: 0.063651, acc:  93%] [G loss: 42.799690] time: 0:02:58.808080\n",
            "[Epoch 0/4] [Batch 302/803] [D loss: 0.145589, acc:  73%] [G loss: 54.107727] time: 0:02:59.303708\n",
            "[Epoch 0/4] [Batch 303/803] [D loss: 0.158427, acc:  88%] [G loss: 36.181332] time: 0:02:59.868659\n",
            "[Epoch 0/4] [Batch 304/803] [D loss: 0.502015, acc:  71%] [G loss: 55.026249] time: 0:03:00.364122\n",
            "[Epoch 0/4] [Batch 305/803] [D loss: 1.647233, acc:  57%] [G loss: 59.972305] time: 0:03:00.846109\n",
            "[Epoch 0/4] [Batch 306/803] [D loss: 0.320272, acc:  71%] [G loss: 42.985390] time: 0:03:01.413622\n",
            "[Epoch 0/4] [Batch 307/803] [D loss: 0.237930, acc:  79%] [G loss: 52.627914] time: 0:03:01.970934\n",
            "[Epoch 0/4] [Batch 308/803] [D loss: 0.332984, acc:  60%] [G loss: 34.038570] time: 0:03:02.469997\n",
            "[Epoch 0/4] [Batch 309/803] [D loss: 0.152488, acc:  83%] [G loss: 40.321598] time: 0:03:02.960078\n",
            "[Epoch 0/4] [Batch 310/803] [D loss: 0.063107, acc:  95%] [G loss: 53.961460] time: 0:03:03.452993\n",
            "[Epoch 0/4] [Batch 311/803] [D loss: 0.075692, acc:  94%] [G loss: 44.757767] time: 0:03:03.940959\n",
            "[Epoch 0/4] [Batch 312/803] [D loss: 0.094993, acc:  90%] [G loss: 33.424183] time: 0:03:04.432744\n",
            "[Epoch 0/4] [Batch 313/803] [D loss: 0.078554, acc:  90%] [G loss: 40.372894] time: 0:03:04.993928\n",
            "[Epoch 0/4] [Batch 314/803] [D loss: 0.048098, acc:  99%] [G loss: 56.136314] time: 0:03:05.559626\n",
            "[Epoch 0/4] [Batch 315/803] [D loss: 0.029694, acc:  99%] [G loss: 39.391350] time: 0:03:06.131081\n",
            "[Epoch 0/4] [Batch 316/803] [D loss: 0.081294, acc:  90%] [G loss: 35.320370] time: 0:03:06.696213\n",
            "[Epoch 0/4] [Batch 317/803] [D loss: 0.082222, acc:  98%] [G loss: 56.721291] time: 0:03:07.264623\n",
            "[Epoch 0/4] [Batch 318/803] [D loss: 0.102515, acc:  84%] [G loss: 26.889982] time: 0:03:07.823523\n"
          ]
        }
      ],
      "source": [
        "gan = Pix2Pix()\n",
        "# gan.train(epochs=200, batch_size=1, sample_interval=200)\n",
        "gan.train(epochs=4, batch_size=1, sample_interval=200)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_img = gan.data_loader.load_batch(1)\n",
        "pred = gan.generator.predict(test_img)"
      ],
      "metadata": {
        "id": "SVSicedynvO2"
      },
      "id": "SVSicedynvO2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_i, (imgs_A, imgs_B) = enumerate(gan.data_loader.load_batch(1))\n",
        "print(imgs_A.shape)"
      ],
      "metadata": {
        "id": "91VsaNeBcq9w"
      },
      "id": "91VsaNeBcq9w",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.imshow(imgs_A[0])"
      ],
      "metadata": {
        "id": "xM7x3k6xeLB_"
      },
      "id": "xM7x3k6xeLB_",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pred = gan.generator.predict(imgs_A)"
      ],
      "metadata": {
        "id": "dM5SeDdkeNbE"
      },
      "id": "dM5SeDdkeNbE",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.imshow(pred[0])"
      ],
      "metadata": {
        "id": "n7T9Xvxaelsz"
      },
      "id": "n7T9Xvxaelsz",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3nehCfqBfPfZ"
      },
      "id": "3nehCfqBfPfZ",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}